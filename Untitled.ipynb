{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (807, 90, 90, 3)\n",
      "y_train:  (807,)\n",
      "X_val:  (100, 90, 90, 3)\n",
      "y_val:  (100,)\n",
      "X_test:  (50, 90, 90, 3)\n",
      "y_test:  (50,)\n",
      "mean_image:  (3, 90, 90)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from core.classifiers.cnn import *\n",
    "from core.data_utils import load_fish_dataset\n",
    "import math\n",
    "\n",
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data_fish_tf = load_fish_dataset(\"/home/pushyamik/Downloads/FishDetection/RockFishDetection\", subtract_mean=False, trans=True)\n",
    "for k, v in data_fish_tf.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'layer5/Wconv:0' shape=(7, 7, 3, 32) dtype=float32_ref>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-beee4ee95150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mdata_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSPropOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# batch normalization in tensorflow requires this extra dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 90, 90, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "class ConvNeuNet(object):\n",
    "    def __init__(self): \n",
    "def build_model(X, y):\n",
    "    \n",
    "\n",
    "    with open(\"core/config.yaml\", 'r') as ymlfile:\n",
    "        cfg = yaml.load(ymlfile)\n",
    "\n",
    "    layers = []\n",
    "    i=0\n",
    "    for k, v in cfg.items():\n",
    "        # go through the layers one by one  \n",
    "        i= i+1\n",
    "        if k == \"learning_rate\":\n",
    "            learn_rate = v\n",
    "        elif k == \"batch_size\":\n",
    "            batch_size = v\n",
    "        elif k == \"learning_decay\":\n",
    "            learn_decay = v\n",
    "        elif k == \"loss\":\n",
    "            loss_type = v['type']\n",
    "            reg = v['reg']\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale = reg)\n",
    "        with tf.variable_scope('layer'+str(i)): \n",
    "            dims = len(X.shape.as_list())\n",
    "            if dims == 4 :\n",
    "                N, H, W, C = X.shape.as_list()\n",
    "            else :\n",
    "                N,H = X.shape.as_list()\n",
    "        \n",
    "            if \"conv\" in k:\n",
    "                #params for conv layer\n",
    "                pad = v['pad']\n",
    "                HH,WW,C = v['filter']\n",
    "                F = v['num_filters']\n",
    "                strideH, strideW = v['stride']\n",
    "            \n",
    "                if pad == \"VALID\":\n",
    "                    H_prime = math.ceil(float(H - HH + 1) / float(strideH))\n",
    "                    W_prime = math.ceil(float(W - WW + 1) / float(strideW))\n",
    "                else:\n",
    "                    H_prime = math.ceil(float(H) / float(strideH))\n",
    "                    W_prime = math.ceil(float(W) / float(strideW))\n",
    "            \n",
    "                s = [HH, WW, C, F]\n",
    "                Wconv = tf.get_variable(\"Wconv\", shape = s, regularizer = regularizer )\n",
    "                bconv = tf.get_variable(\"bconv\", shape = F)\n",
    "                X = tf.nn.conv2d(X, Wconv, strides=[1,strideH, strideW,1], padding = pad) + bconv\n",
    "            \n",
    "                if v['bn']:\n",
    "                    #params or batch normalization \n",
    "                    scale_conv = tf.get_variable(\"scale_conv\", shape=[H_prime, W_prime, F])\n",
    "                    offset_conv = tf.get_variable(\"offset_conv\", shape=[H_prime, W_prime, F])\n",
    "                \n",
    "                    #batchnormaization for conv layer\n",
    "                    mean_conv, var_conv = tf.nn.moments(X, axes=[0], keep_dims = False)\n",
    "                    X = tf.nn.batch_normalization(X, mean_conv, var_conv, offset_conv, scale_conv, 1e-6)\n",
    "                #Relu \n",
    "                X = tf.nn.relu(X)\n",
    "                layers.append(X)\n",
    "                print(Wconv)\n",
    "        \n",
    "            elif k == \"maxpool\":\n",
    "                #max pooling\n",
    "                kH, kW = v['size']\n",
    "                strideH, strideW = v['stride']\n",
    "                X = tf.nn.max_pool(X, ksize=[1,kH,kW,1], strides=[1,strideH, strideW,1], padding='VALID', data_format='NHWC')      \n",
    "                layers.append(X)\n",
    "            \n",
    "            elif k == \"affine\":\n",
    "                outputs = v['outputs']\n",
    "             \n",
    "                #affine layer \n",
    "                if dims == 4:\n",
    "                    X = tf.reshape(X,[-1 , H * W * C])\n",
    "                    #params for affine layer with 1024 outputs\n",
    "                    W = tf.get_variable(\"W\", shape=[H * W * C, outputs],  regularizer = regularizer)\n",
    "                else:\n",
    "                    W =tf.get_variable(\"W\", shape=[H, outputs],  regularizer = regularizer) \n",
    "                \n",
    "                b = tf.get_variable(\"b\", shape=[outputs])\n",
    "                X = tf.matmul(X , W) + b\n",
    "         \n",
    "            \n",
    "                if v['bn']:\n",
    "                    #params for batch normalization for affine\n",
    "                    scale_affine = tf.get_variable(\"scale_affine\", shape=[outputs])\n",
    "                    offset_affine = tf.get_variable(\"offset_affine\", shape=[outputs])\n",
    "                                               \n",
    "                    #batchnormalization for affine layer\n",
    "                    mean_aff, var_aff = tf.nn.moments(X, axes=[0], keep_dims=False)\n",
    "                    X = tf.nn.batch_normalization(X, mean_aff, var_aff, offset_affine, scale_affine, 1e-6)\n",
    "            \n",
    "                if v['activation'] == \"relu\":\n",
    "                    #relu\n",
    "                    X = tf.nn.relu(X)  \n",
    "                layers.append(X)\n",
    "    y_out = X                                             \n",
    "    reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) \n",
    "    reg_loss = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "    return y_out, reg_loss                                          \n",
    "\n",
    "yout, reg_loss = build_model(X, y)                                                   \n",
    "data_loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(tf.one_hot(y,2), logits=yout))\n",
    "loss = data_loss + reg_loss\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_out,loss,data_fish_tf['X_train'],data_fish_tf['y_train'],1,20,10,train_step,True,0.001)\n",
    "        print('Validation')\n",
    "        run_model(sess,y_out,loss,data_fish_tf['X_val'],data_fish_tf['y_val'],1,40)\n",
    "\n",
    "\n",
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False, reg_param = 0.001):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print (type(Xd))\n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [loss_val,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    print(variables)\n",
    "    \n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx] }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct\n",
    "\n",
    "#for i in range(10):\n",
    "#    with tf.variable_scope(\"scope\"+str(i)):\n",
    "#        #add a new variable to the graph\n",
    "#        var = tf.get_variable(\"variable1\",[1])\n",
    "#        #print the name of variable\n",
    "#        print(var.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
